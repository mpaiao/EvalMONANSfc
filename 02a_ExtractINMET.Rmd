---
title: "Pre-process INMET data"
author: "Marcos Longo"
date: "2026-02-26"
output: html_document
---


<style>
pre {
  overflow-x: auto;
}
pre code {
  word-wrap: normal;
  white-space: pre;
}
</style>


```{r, label= 'setup', include=FALSE}
# Reduce the amount of output displayed in the knitted version
knitr::opts_chunk$set(message=FALSE, warning = FALSE, results='hide', collapse = TRUE)
```


# Introduction

This R Markdown helps pre-process automatic weather station data from Brazil's National Institute of Meteorology (INMET). Data are available for download by year at [INMET's meteorological data base portal](https://bdmep.inmet.gov.br). This script has been tested with data downloaded in bulk from the first link available (web site in Portuguese). 

This script requires some functions available in sub-directory `RUtils`, which is available in the repository. 


# Reset session

Use this chunk to fully reset R.
```{r, label = 'reset-R',message=FALSE, results='hide', collapse = TRUE}
# Unload all packages except for the R default ones
plist = names(sessionInfo()$otherPkgs)
if (length(plist) > 0){
   dummy = sapply(X=paste0("package:",plist),FUN=detach,character.only=TRUE,unload=TRUE)
}#end if (length(plist) > 0)


# Remove all variables
rm(list=ls())

# Reset warnings
options(warn=0)

# Close all plots
invisible(graphics.off())

# Clean up
invisible(gc())
```

# User configuration 


In this part, we set a few useful global paths, files, and variables.

First, we set some global paths and files:

* **home_path**. Typically the user's home path.  Useful for building other paths. `path.expand("~")` typically works for all users.
* **main_path**. Typically the main directory where all MONAN's surface evaluation scripts are located.
* **observed_path**. The main directory containing text files with all observations. Typically the main path that will host both the raw and the consolidated INMET files.
* **input_path**. The location where the original INMET files are located. Following INMET's bulk download, files should be stored in one sub-directory for each year.
* **output_path**. The main output path for the data. A sub-directory for this site will be created.
* **util_path**. The path with the additional utility scripts (the full path of `RUtils`).

```{r, label='path-settings'}
home_path     = path.expand("~")
main_path     = file.path(home_path,"Documents","LocalData","EvalMONANSfc")
observed_path = file.path(main_path,"Observations","INMET")
util_path     = file.path(main_path, "RUtils")
input_path    = file.path(observed_path,"Original")
output_path   = file.path(observed_path,"Consolidated")
```

We then set the first (`whena`) and last (`whenz`) times to be processed. Provide both in format `"YYYY-MM-DD"`.

```{r, label='time-settings'}
whena = "2025-11-01" # First date ("A")
whenz = "2026-02-28" # Last date ("Z")
```



Define some lower and upper bounds for meteorological variables. These should be vectors with length 2, representing the lower and upper bound of what we consider physically possible and that will not give trouble when deriving the other quantities. Please refrain from making the range too strict and narrow, to avoid unintentionally discarding data. Note that the range should be defined in the same units as MONAN:

* **mslp_bnds**. Bounds for mean sea level pressure, in $\mathrm{Pa}$.
* **acswdnb_bnds**. Bounds for accumulated downward shortwave irradiance, in $\mathrm{J}\,\mathrm{m}^{-2}$.
* **t2m_bnds**. Bounds for 2-m temperature, in $\mathrm{K}$.
* **td2m_bnds**. Bounds for 2-m dew point temperature, in $\mathrm{K}$.
* **ws10_bnds**. Bounds for 10-m wind speed, in $\mathrm{m}\,\mathrm{s}^{-1}$.
* **wd10_bnds**. Bounds for 10-m wind direction, in $^{\circ}$.

```{r,label='met-bounds'}
mslp_bnds    = c(  85000.,  110000.) # Pa
acswdnb_bnds = c(      0., 4932000.) # J/m2
t2m_bnds     = c(    180.,     335.) # K
td2m_bnds    = c(    180.,     335.) # K
ws10_bnds    = c(      0.,     200.) # m/s
wd10_bnds    = c(      0.,     360.) # deg
```

# Main script

__Note:__ Changes beyond this point are only needed if you are developing the notebook.

## Initial settings

First, we load some useful packages.
```{r, label='load-packages'}
cat(" + Load required packages.\n")
# Load all required packages
isfine = 
   c( data.table   = require(data.table)
    , extraDistr   = require(extraDistr)
    , extrafont    = require(extrafont)
    , ggstar       = require(ggstar)
    , grDevices    = require(grDevices)
    , maps         = require(maps)
    , MASS         = require(MASS)
    , patchwork    = require(patchwork)
    , RColorBrewer = require(RColorBrewer)
    , scales       = require(scales)
    , sf           = require(sf)
    , sn           = require(sn)
    , tidyverse    = require(tidyverse)
    , viridis      = require(viridis)
    )#end c

# Check that all packages were successfully loaded.
if (any(! isfine)){
   cat("---~---\n")
   cat("   FATAL ERROR!!! \n")
   cat("---~---\n")
   cat(" The following packages are needed but could not be loaded:\n")
   cat(" - ",paste(names(isfine)[! isfine],collapse=", "),".\n",sep="")
   cat("---~---\n")
   stop(" Missing required packages")      
}#end if (any(! isfine))

```

We then load all R scripts in the utilities directory.
```{r, label='load-scripts'}
cat(" + Load additional R scripts.\n")
# List all R scripts, but exclude those likely to be backups.
script_list = sort(list.files(path=util_path,pattern="\\.[Rr]$"))
backup_list = sort(list.files(path=util_path,pattern="^[~]"))
script_list = script_list[! script_list %in% backup_list]

# Load all files and make sure they are alright.
warn_orig = getOption("warn")
options(warn=2)
for ( script_base in script_list){
   script_file = file.path(util_path,script_base)
   success = try(source(script_file,chdir=TRUE),silent=TRUE)
   if ("try-error" %in% is(success)){
      options(warn=warn_orig)
      cat("---~---\n")
      cat("   FATAL ERROR!!! \n")
      cat("---~---\n")
      cat("   Script ",script_base," has bugs! Check the errors/warnings:\n",sep="")
      cat("---~---\n")
      source(script_file,chdir=TRUE)
      stop(" Source code problem.")      
   }#end if ("try-error" %in% is(success))
}#end for ( script_file in script_list)
options(warn=warn_orig)
```

We create the output path in case it is not there.

```{r,label='make-output-path'}
cat0(" + Make sure the output path exists.")
dummy = dir.create(output_path,recursive=TRUE,showWarnings=FALSE)
```

We then set times for looping through times.

```{r,label='set-time-span'}
cat0(" + Set time bounds and time steps.")
whena    = as.integer(strsplit(x=whena,split="-")[[1L]])
whenz    = as.integer(strsplit(x=whenz,split="-")[[1L]])
whena    = lubridate::make_datetime( year = whena[1L], month = whena[2L], day = whena[3L]
                                   , hour = 0L       , min   = 0L       , sec = 0L
                                   , tz   = "UTC"    )
whenz    = lubridate::make_datetime( year = whenz[1L], month = whenz[2L], day = whenz[3L]
                                   , hour = 23L      , min   = 59L      , sec = 59L
                                   , tz   = "UTC"    )
yeara    = lubridate::year(whena)
yearz    = lubridate::year(whenz)
when_seq = seq(from=whena,to=whenz,by="1 hour")
```

We then find the expected range of water vapour partial pressure, which in turn will be used for finding a range for surface pressure. The range is defined by the lowest temperature bound and 0.1% relative humidity, and the highest temperature bound and 100% relative humidity.

The bounds for surface pressure must be found later, because they depend on the elevation of each site.

```{r,label='find-pvap2m-bnds'}
pvap2m_bnds = c( 0.001 * t2m_to_pvsat2m(t2m=t2m_bnds[1L]), t2m_to_pvsat2m(t2m=t2m_bnds[2L]) )
```

Finally, we set up an information tibble object that will summarise all stations available.

```{r,label='init-info-table'}
# Initialise information table.
inmet_info = tibble( ident     = character(0L)
                   , lon       = numeric  (0L)
                   , lat       = numeric  (0L)
                   , alt       = numeric  (0L)
                   , site      = character(0L)
                   , state     = character(0L)
                   , last_year = integer  (0L)
                   )#end tibble
```


# Main data processing.


In this loop, we perform the following tasks:
1. Retrieve original data as it looks like from INMET. 
2. Obtain the meta-data and place them in the summary tibble object.
3. Apply a very basic QA/QC (values within physically meaningful ranges)
4. Find a few derived thermodynamic and meteorological variables. Most derived quantities use functions defined in script `RUtils/thermo_phys_library.r`.
5. Trim data to the period of interest and append the data to a global structure to a tibble containing all data.

```{r,label='main-processing'}
# Loop through years.
inmet_data = NULL
for (year in seq(from=yeara,to=yearz,by=1L)){
   # List all files.
   cat0(" + Process data for ",year,".")
   year_path       = file.path (input_path,year)
   inmet_file_list = list.files( path        = year_path
                               , pattern     = "\\.csv$"
                               , ignore.case = TRUE
                               , full.names  = TRUE
                               )#end list.files
   inmet_file_list = sort(inmet_file_list)


   # Loop through all files.
   for (inmet_file in inmet_file_list){
      # Open file and read the header
      inmet_header = suppressMessages( read_csv2( file           = inmet_file
                                                , n_max          = 8L
                                                , col_names      = FALSE
                                                , show_col_types = FALSE
                                                , progress       = FALSE
                                                )#end read_csv2
                                     )#end suppressMessages
      inmet_state  = str_to_upper(inmet_header$X2[2L])
      inmet_site   = str_to_title(inmet_header$X2[3L])
      inmet_ident  = str_to_upper(inmet_header$X2[4L])
      inmet_lat    = as.numeric(gsub(pattern="\\,",replacement=".",x=inmet_header$X2[5L]))
      inmet_lon    = as.numeric(gsub(pattern="\\,",replacement=".",x=inmet_header$X2[6L]))
      inmet_alt    = as.numeric(gsub(pattern="\\,",replacement=".",x=inmet_header$X2[7L]))
      cat0("   - Site ",inmet_site," ",inmet_state," (",inmet_ident,").")


      # Append site if it is not there already.
      if (inmet_ident %in% inmet_info$ident){
         ih = match(inmet_ident,inmet_info$ident)
         if (year == inmet_info$last_year[ih]){
            cat0("")
            cat0("---~---")
            cat0("   FATAL ERROR!!!")
            cat0("---~---")
            cat0("   + Site ",inmet_site," ",inmet_state," (",inmet_ident,").")
            cat0("   + This site is duplicated for year ",year,"!!")
            cat0("---~---")
            stop(" Sites should have a single file per year.")
         }else{
            cat0("     ~ Site is already in the look-up table. Update last year")
            inmet_info$last_year[ih] = year
         }#end if (year == inmet_info$last_year[ih])
      }else{
         cat0("     ~ Add site to the look-up table.")
         this_info  = tibble( ident     = inmet_ident
                            , lon       = inmet_lon
                            , lat       = inmet_lat
                            , alt       = inmet_alt
                            , site      = inmet_site
                            , state     = inmet_state
                            , last_year = year
                            )#end tibble
         inmet_info = rbind(inmet_info,this_info)
      }#end if (! inmet_ident %in% inmet_info$ident)

      # Load data.
      this_datum = suppressMessages( read_csv2( file           = inmet_file
                                              , skip           = 9L
                                              , col_names      = FALSE
                                              , col_types      = "ccnnnnnnnnnnnnnnnnnc"
                                              , show_col_types = FALSE
                                              )#end read_csv2
                                   )#end suppressMessages
      names(this_datum) = c( "date_str", "time_str","rain","surface_pressure"
                           , "min_surface_pressure", "max_surface_pressure"
                           , "acswdnb","t2m","td2m","max_t2m","min_t2m","max_td2m"
                           , "min_td2m","max_rh2m","min_rh2m","rh2m","wd10","max_ws10"
                           , "ws10","dummy")

      # Standardise time
      this_datum = this_datum %>%
         mutate( year  = as.integer( substr( x = date_str, start = 1L, stop =  4L ) )
               , month = as.integer( substr( x = date_str, start = 6L, stop =  7L ) )
               , day   = as.integer( substr( x = date_str, start = 9L, stop = 10L ) )
               , hour  = as.integer( substr( x = time_str, start = 1L, stop =  2L ) )
               , min   = as.integer( substr( x = time_str, start = 3L, stop =  4L ) )
               , when  = make_datetime( year = year, month = month, day = day
                                      , hour = hour, min   = min  , sec = 0L
                                      , tz   = "UTC")
               )#end mutate


      # Add station information
      this_datum = this_datum %>%
         mutate( ident   = inmet_ident
               , site    = inmet_site
               , lon     = inmet_lon
               , lat     = inmet_lat
               , ter     = inmet_alt
               , state   = inmet_state
               )#end mutate

      # Make sure the units match those used in MONAN.
      this_datum = this_datum %>%
         mutate( surface_pressure = surface_pressure * hPa_to_Pa
               , acswdnb          = acswdnb * kJ_to_J
               , t2m              = t2m  + t00
               , td2m             = td2m + t00
               )#end mutate


      #    Find the range for surface pressure. The low-pressure bound is found with the lowest bound
      # for all three dependent variables (mean surface-level pressure, temperature, vapour pressure).
      # Likewise, the high-pressure bound uses the upper bound of these variables. This is because
      # theas they are all positively correlated with pressure. 
      surface_pressure_bnds = find_sfc_pressure(mslp=mslp_bnds,t2m=t2m_bnds,pvap2m=pvap2m_bnds,ter=inmet_alt)

      
      # Discard data outside reasonable range.
      this_datum = this_datum %>%
         mutate( surface_pressure = check_bounds( x = surface_pressure, xBounds = surface_pressure_bnds)
               , acswdnb          = check_bounds( x = acswdnb         , xBounds = acswdnb_bnds         )
               , t2m              = check_bounds( x = t2m             , xBounds = t2m_bnds             )
               , td2m             = check_bounds( x = td2m            , xBounds = td2m_bnds            )
               , ws10             = check_bounds( x = ws10            , xBounds = ws10_bnds            )
               , wd10             = check_bounds( x = wd10            , xBounds = wd10_bnds            )
               )#end mutate
      
      # Find derived quantities
      this_datum = this_datum %>%
         mutate( pvap2m = t2m_to_pvsat2m(t2m=td2m)
               , q2     = find_q2m(surface_pressure,t2m,pvap2m)
               , u10    = find_u10(ws10,wd10)
               , v10    = find_v10(ws10,wd10)
               , tv2m   = find_tv2m(surface_pressure,t2m,pvap2m)
               , mslp   = find_mslp(surface_pressure, t2m, pvap2m, ter)
               )#end mutate

      # Select variable to carry out.
      cat0("     ~ Trim data to variables and period of interest.")
      this_datum = this_datum                              %>%
         select(when,ident,site,lon,lat,ter
               ,surface_pressure,mslp,acswdnb,u10,v10
               ,q2,t2m,pvap2m,td2m,rain                  ) %>%
         filter( ( when >= whena ) & ( when <= whenz ) )

      # Append data to the global data set.
      inmet_data = rbind(inmet_data,this_datum)
   }#end for (inmet_file in inmet_file_list)
}#end for (yearn in seq(from=yeara,to=yearz,by=1L))
```

Sort all data by time and station.

```{r,label='sort-data'}
cat0(" + Sort data by time and station.")
inmet_info = inmet_info %>%
   arrange(ident)
inmet_data = inmet_data %>%
   arrange(ident,when)
```


# Write site-specific csv files

We make some final adjustments to the data sets, such as including all times within the period of interest, and write standardised site-specific csv files with all files. We also count the number of valid measurements for each site and variable, and add a few pieces of information to the site list summary.

```{r,label='write-site-csv'}
# Add variables that will tally the number of valid observations.
cat0(" + Append observation count variables to the summary table.")
inmet_info = inmet_info                                              %>%
   select(ident,lon,lat,alt,site,state)                              %>%
   mutate( n_surface_pressure = NA_integer_
         , n_mslp             = NA_integer_
         , n_acswdnb          = NA_integer_
         , n_u10              = NA_integer_
         , n_v10              = NA_integer_
         , n_q2               = NA_integer_
         , n_t2m              = NA_integer_
         , n_pvap2m           = NA_integer_
         , n_td2m             = NA_integer_
         , n_rain             = NA_integer_ )                        %>%
   select(ident,lon,lat,alt,site,state,n_surface_pressure
         ,n_mslp,n_acswdnb,n_u10,n_v10,n_q2,n_t2m,n_pvap2m,n_td2m
         ,n_rain                                                  )


# Extract data for each site and produce site-specific csv files with no time gaps.
cat0(" + Write csv files for each individual site.")
n_inmet    = nrow(inmet_info)
for (i in sequence(n_inmet)){
   # Pick information.
   i_ident    = inmet_info$ident [i]
   i_site     = inmet_info$site  [i]
   i_lon      = inmet_info$lon   [i]
   i_lat      = inmet_info$lat   [i]
   i_alt      = inmet_info$alt   [i]
   i_state    = inmet_info$state [i]
   i_label    = gsub(pattern=" ",replacement="",x=i_site)

   # Define output file.
   inmet_base  = paste0("inmet_",i_ident,"_",i_label,".csv")
   inmet_file  = file.path(output_path,inmet_base)
   cat0("   - Site: ",i_site," ( ",i_ident,").")

   # Initialise METAR file containing all times.
   inmet_output = tibble( when             = when_seq
                        , ident            = i_ident
                        , site             = i_site
                        , lon              = i_lon
                        , lat              = i_lat
                        , ter              = i_alt
                        , state            = i_state
                        , surface_pressure = NA_real_
                        , mslp             = NA_real_
                        , acswdnb          = NA_real_
                        , u10              = NA_real_
                        , v10              = NA_real_
                        , q2               = NA_real_
                        , t2m              = NA_real_
                        , pvap2m           = NA_real_
                        , td2m             = NA_real_
                        , rain             = NA_real_
                        )#end tibble

   # Subset global tibble and write the site-specific data.
   this_inmet = inmet_data %>%
      filter( ident %in% i_ident )

   # Find the number of valid data points.
   inmet_info$n_surface_pressure[i] = sum(is.finite(this_inmet$surface_pressure))
   inmet_info$n_mslp            [i] = sum(is.finite(this_inmet$mslp            ))
   inmet_info$n_acswdnb         [i] = sum(is.finite(this_inmet$acswdnb         ))
   inmet_info$n_u10             [i] = sum(is.finite(this_inmet$u10             ))
   inmet_info$n_v10             [i] = sum(is.finite(this_inmet$v10             ))
   inmet_info$n_q2              [i] = sum(is.finite(this_inmet$q2              ))
   inmet_info$n_t2m             [i] = sum(is.finite(this_inmet$t2m             ))
   inmet_info$n_pvap2m          [i] = sum(is.finite(this_inmet$pvap2m          ))
   inmet_info$n_td2m            [i] = sum(is.finite(this_inmet$td2m            ))
   inmet_info$n_rain            [i] = sum(is.finite(this_inmet$rain            ))

   # Find available times for this site.
   ii = match(this_inmet$when,inmet_output$when)
   inmet_output$surface_pressure[ii] = this_inmet$surface_pressure
   inmet_output$mslp            [ii] = this_inmet$mslp
   inmet_output$acswdnb         [ii] = this_inmet$acswdnb
   inmet_output$u10             [ii] = this_inmet$u10
   inmet_output$v10             [ii] = this_inmet$v10
   inmet_output$q2              [ii] = this_inmet$q2
   inmet_output$t2m             [ii] = this_inmet$t2m
   inmet_output$pvap2m          [ii] = this_inmet$pvap2m
   inmet_output$td2m            [ii] = this_inmet$td2m
   inmet_output$rain            [ii] = this_inmet$rain

   # Write csv file
   dummy = write_csv( x         = inmet_output
                    , file      = inmet_file
                    , na        = ""
                    , append    = FALSE
                    , col_names = TRUE
                    )#end write_csv
}#end for (m in sequence(n_metar))
```


# Summary table output

Lastly, we write the summary to a csv file. This will be useful for extracting model results for each of the sites.

```{r,label='write-summary-table'}
info_base = "INMET_SiteInfo.csv"
info_csv  = file.path(observed_path,info_base)
cat0(" + Write site table summary to ",info_base,".")
dummy     = write_csv( x         = inmet_info
                     , file      = info_csv
                     , na        = ""
                     , append    = FALSE
                     , col_names = TRUE
                     )
```
